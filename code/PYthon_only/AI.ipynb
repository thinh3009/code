{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- khai niem yolov8: \n",
    "+ la phien ban moi nhat cua xu ly thi giac may tinh va hinh anh\n",
    "+ duoc xay dung trong deep learning va thi giac may tinh uu tien ve toc do va do chinh xac\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* o lab2 nay nhom thuc hien de tai: phat hien va danh gia hieu qua cua cac doi tuong dua vao yolov8 va kerasCv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Septup: goi cac thu vien can thiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import keras_cv\n",
    "from keras_cv import bounding_box\n",
    "from keras_cv import visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_RATIO = 0.2\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCH = 5\n",
    "GLOBAL_CLIPNORM = 10.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Dictionary tao tung ten lop voi ma dinh danh duy nhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/dataset/data/annotations/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m path_annot \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/dataset/data/annotations/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Get all XML file paths in path_annot and sort them\u001b[39;00m\n\u001b[1;32m     15\u001b[0m xml_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m     16\u001b[0m     [\n\u001b[1;32m     17\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_annot, file_name)\n\u001b[0;32m---> 18\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_annot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m     ]\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Get all JPEG image file paths in path_images and sort them\u001b[39;00m\n\u001b[1;32m     24\u001b[0m jpg_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m     25\u001b[0m     [\n\u001b[1;32m     26\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_images, file_name)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     ]\n\u001b[1;32m     30\u001b[0m )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/dataset/data/annotations/'"
     ]
    }
   ],
   "source": [
    "class_ids = [\n",
    "    \"car\",\n",
    "    \"pedestrian\",\n",
    "    \"trafficLight\",\n",
    "    \"biker\",\n",
    "    \"truck\",\n",
    "]\n",
    "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
    "\n",
    "# Path to images and annotations\n",
    "path_images = \"/kaggle/input/dataset/data/images/\"\n",
    "path_annot = \"/kaggle/input/dataset/data/annotations/\"\n",
    "\n",
    "# Get all XML file paths in path_annot and sort them\n",
    "xml_files = sorted(\n",
    "    [\n",
    "        os.path.join(path_annot, file_name)\n",
    "        for file_name in os.listdir(path_annot)\n",
    "        if file_name.endswith(\".xml\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Get all JPEG image file paths in path_images and sort them\n",
    "jpg_files = sorted(\n",
    "    [\n",
    "        os.path.join(path_images, file_name)\n",
    "        for file_name in os.listdir(path_images)\n",
    "        if file_name.endswith(\".jpg\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ham cho phep doc file xml va tim kiem ten anh va path\n",
    "ham se tra ve 3 gia tri: anh, duong dan, cac khoi block dc bieu thi dang (xmin, ymin, xmax, ymax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotation(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    image_name = root.find(\"filename\").text\n",
    "    image_path = os.path.join(path_images, image_name)\n",
    "\n",
    "    boxes = []\n",
    "    classes = []\n",
    "    for obj in root.iter(\"object\"):\n",
    "        cls = obj.find(\"name\").text\n",
    "        classes.append(cls)\n",
    "\n",
    "        bbox = obj.find(\"bndbox\")\n",
    "        xmin = float(bbox.find(\"xmin\").text)\n",
    "        ymin = float(bbox.find(\"ymin\").text)\n",
    "        xmax = float(bbox.find(\"xmax\").text)\n",
    "        ymax = float(bbox.find(\"ymax\").text)\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "    class_ids = [\n",
    "        list(class_mapping.keys())[list(class_mapping.values()).index(cls)]\n",
    "        for cls in classes\n",
    "    ]\n",
    "    return image_path, boxes, class_ids\n",
    "\n",
    "\n",
    "image_paths = []\n",
    "bbox = []\n",
    "classes = []\n",
    "for xml_file in tqdm(xml_files):\n",
    "    image_path, boxes, class_ids = parse_annotation(xml_file)\n",
    "    image_paths.append(image_path)\n",
    "    bbox.append(boxes)\n",
    "    classes.append(class_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    [8, 8, 8, 8, 8],      # 5 classes\n",
    "    [12, 14, 14, 14],     # 4 classes\n",
    "    [1],                  # 1 class\n",
    "    [7, 7],               # 2 classes\n",
    " ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = [\n",
    "    [[199.0, 19.0, 390.0, 401.0],\n",
    "    [217.0, 15.0, 270.0, 157.0],\n",
    "    [393.0, 18.0, 432.0, 162.0],\n",
    "    [1.0, 15.0, 226.0, 276.0],\n",
    "    [19.0, 95.0, 458.0, 443.0]],     #image 1 has 4 objects\n",
    "    [[52.0, 117.0, 109.0, 177.0]],   #image 2 has 1 object\n",
    "    [[88.0, 87.0, 235.0, 322.0],\n",
    "    [113.0, 117.0, 218.0, 471.0]],   #image 3 has 2 objects\n",
    " ...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. dung tf.rangged.constrant de tao 2 bien bbox va classes chua 2 chuoi o tren\n",
    "- rangged: co the xu ly do dai du lieu khac nhau theo 1 chieu hay nhieu chieu--> no rat huu ich khi xu ly du lieu co chuoi co do dai thay doi vd nhu van ban, tgian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. o truong hopj nay 2 ben bbox va classes co do dai khac biet cho moi anh\n",
    "- dua vao so luong doi tuong trong anh co the dong khung cac doi tuong tuong ung\n",
    "- sau do tensors rangged nay duoc su dung de tao tf.data.Dataset bang method from_tensor_slices.\n",
    "--> phuong thuc nay tensors co the xu ly cac do dai du lieu khac nhau voi moi hinh anh duoc cung cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = tf.ragged.constant(bbox)\n",
    "classes = tf.ragged.constant(classes)\n",
    "image_paths = tf.ragged.constant(image_paths)\n",
    "\n",
    "data = tf.data.Dataset.from_tensor_slices((image_paths, classes, bbox))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. tach du lieu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of validation samples\n",
    "num_val = int(len(xml_files) * SPLIT_RATIO)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "val_data = data.take(num_val)\n",
    "train_data = data.skip(num_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. de gioi han dinh dang dong hop cac hop gioi han trong Keras phai do dinh dang xac dinh truoc vi vay phai dong goi hop vao dictionnary nhu code ben duoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. dictionary co 2 keys: 'boxes' va 'classes' moi keys anh xa toi TensorFlow RaggedTensor or Tensor object. \n",
    "trong do:\n",
    "+ boxes: (bacth va num_boxes) la noi chua so luong toi da gioi han hop trong bat ki anh nao, 4 dai dien cho 4 gia tri can thiet de xac dinh xmin,ymin,xmax,ymax\n",
    "+ classes: (batch va num_boxes) dai dien cho label cho hop gioi han trong boxes, kich thuoc cua hop khong deu nhau nghiax la so luong hop co the khac nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_boxes = {\n",
    "    # num_boxes may be a Ragged dimension\n",
    "    'boxes': Tensor(shape=[batch, num_boxes, 4]),\n",
    "    'classes': Tensor(shape=[batch, num_boxes])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"images\": images, \"bounding_boxes\": bounding_boxes}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_dataset(image_path, classes, bbox):\n",
    "    # Read Image\n",
    "    image = load_image(image_path)\n",
    "    bounding_boxes = {\n",
    "        \"classes\": tf.cast(classes, dtype=tf.float32),\n",
    "        \"boxes\": bbox,\n",
    "    }\n",
    "    return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. o day tao cac layer de resizes img thanh 640x640 pixels trong khi van duy tri ti le goc, cac hop goi han lien ket voi hinh anh duoc chi dinh dang xyxy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = keras_cv.bounding_box.convert_format(\n",
    "        bounding_box,\n",
    "        images=image,\n",
    "        source=\"xyxy\",  # Original Format\n",
    "        target=\"xywh\",  # Target Format (to which we want to convert)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. khi xay dung pipline dectection dieu kho khan nhat la viec tang cuong du lieu. no lien quan den viec ap dung phep bien doi khac nhau de tang tinh da dang cua du lieu va kha nang khai quat cua mo hinh\n",
    "- Keras cung cap cac ho tro cho viec tang cuong hop gioi han. KerasCV cung cap mot bo suu tap phong phu cac lop tang cuongdu lieu de xu ly cac hop gioi han\n",
    "- cac lop nay dieu chinh toa do hop mot cach thong minh khi hinh anh duoc chuyen doi de dam bao can chinh chinh xac hinh anh\\\n",
    "- tan dung kha nang cua KerasCV co the thuc hien tang cuong nhanh chong trong pipeline tf.data, no se tro nen lien mach va hieu qua cho phe train tot hon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = keras.Sequential(\n",
    "    layers=[\n",
    "        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xyxy\"),\n",
    "        keras_cv.layers.RandomShear(\n",
    "            x_factor=0.2, y_factor=0.2, bounding_box_format=\"xyxy\"\n",
    "        ),\n",
    "        keras_cv.layers.JitteredResize(\n",
    "            target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format=\"xyxy\"\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tao tranning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(BATCH_SIZE * 4)\n",
    "train_ds = train_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resizing = keras_cv.layers.JitteredResize(\n",
    "    target_size=(640, 640),\n",
    "    scale_factor=(0.75, 1.3),\n",
    "    bounding_box_format=\"xyxy\",\n",
    ")\n",
    "\n",
    "val_ds = val_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.shuffle(BATCH_SIZE * 4)\n",
    "val_ds = val_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_ds = val_ds.map(resizing, num_parallel_calls=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n",
    "    inputs = next(iter(inputs.take(1)))\n",
    "    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=value_range,\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        y_true=bounding_boxes,\n",
    "        scale=5,\n",
    "        font_scale=0.7,\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        class_mapping=class_mapping,\n",
    "    )\n",
    "\n",
    "\n",
    "visualize_dataset(\n",
    "    train_ds, bounding_box_format=\"xyxy\", value_range=(0, 255), rows=2, cols=2\n",
    ")\n",
    "\n",
    "visualize_dataset(\n",
    "    val_ds, bounding_box_format=\"xyxy\", value_range=(0, 255), rows=2, cols=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ta can xac dinh dau vao tu preprocesssing dictionary va lay no de san san dua vao model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_tuple(inputs):\n",
    "    return inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating Model\n",
    "- YOLOv8 là mô hình YOLO tiên tiến được sử dụng cho nhiều tác vụ thị giác máy tính khác nhau, chẳng hạn như phát hiện đối tượng, phân loại hình ảnh và phân đoạn phiên bản. \n",
    "- Ultralytics, người sáng tạo ra YOLOv5, cũng đã phát triển YOLOv8, kết hợp nhiều cải tiến và thay đổi về kiến ​​trúc cũng như trải nghiệm của nhà phát triển so với phiên bản tiền nhiệm. \n",
    "- YOLOv8 là model tiên tiến mới nhất được đánh giá cao trong ngành."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. With Pre-trained coco weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = keras_cv.models.YOLOV8Backbone.from_preset(\n",
    "    \"yolo_v8_s_backbone_coco\"  # We will use yolov8 small backbone with coco weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tiep theo xay dung yoloV8 model bang cach su dung YOLOV8Detector \n",
    "* trong do\n",
    "+ num_classes: chi dinh so luong lop doi tuong can phat hien dua tren kich thuoc danh sach class_mapping\n",
    "+ bounding_box_form: thong bao cho mo hinh ve dinh dang cuar bbox trong tap du lieu\n",
    "+fpn_depth: do sau "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = keras_cv.models.YOLOV8Detector(\n",
    "    num_classes=len(class_mapping),\n",
    "    bounding_box_format=\"xyxy\",\n",
    "    backbone=backbone,\n",
    "    fpn_depth=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compile the Model\n",
    "- Classification Loss: ham nay tinh toan su khac biet giua xac suat lop du doan va xac suat lop trong thuc te, trong truong hop nay 'binary_crossebtropy' la giai phap phan loai nhi phan duoc su dung,\n",
    "- Box Loss: dung do luong su khac biet giua hop gioi han duoc du doan, truong hop nay so lieu Complete IoU(CloU) khong chi do su chong cheo giua cac hop gioi han dc du doan thuc te maf con xem xet su khac biet ve ti le khung hinh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    global_clipnorm=GLOBAL_CLIPNORM,\n",
    ")\n",
    "\n",
    "yolo.compile(\n",
    "    optimizer=optimizer, classification_loss=\"binary_crossentropy\", box_loss=\"ciou\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* COCO Metric Callback\n",
    "dung BoxCOCOMetrics tu KerasCV de danh gia mo hinh va tinh do chinh xac trung binh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateCOCOMetricsCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, data, save_path):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.metrics = keras_cv.metrics.BoxCOCOMetrics(\n",
    "            bounding_box_format=\"xyxy\",\n",
    "            evaluate_freq=1e9,\n",
    "        )\n",
    "\n",
    "        self.save_path = save_path\n",
    "        self.best_map = -1.0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self.metrics.reset_state()\n",
    "        for batch in self.data:\n",
    "            images, y_true = batch[0], batch[1]\n",
    "            y_pred = self.model.predict(images, verbose=0)\n",
    "            self.metrics.update_state(y_true, y_pred)\n",
    "\n",
    "        metrics = self.metrics.result(force=True)\n",
    "        logs.update(metrics)\n",
    "\n",
    "        current_map = metrics[\"MaP\"]\n",
    "        if current_map > self.best_map:\n",
    "            self.best_map = current_map\n",
    "            self.model.save(self.save_path)  # Save the model when mAP improves\n",
    "\n",
    "        return logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=3,\n",
    "    callbacks=[EvaluateCOCOMetricsCallback(val_ds, \"model.h5\")],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* visual Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(model, dataset, bounding_box_format):\n",
    "    images, y_true = next(iter(dataset.take(1)))\n",
    "    y_pred = model.predict(images)\n",
    "    y_pred = bounding_box.to_ragged(y_pred)\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=(0, 255),\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        scale=4,\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        show=True,\n",
    "        font_scale=0.7,\n",
    "        class_mapping=class_mapping,\n",
    "    )\n",
    "\n",
    "\n",
    "visualize_detections(yolo, dataset=val_ds, bounding_box_format=\"xyxy\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
